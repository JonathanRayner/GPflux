{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "import gpflux\n",
    "import numpy as np\n",
    "\n",
    "# from: https://github.com/hughsalimbeni/bayesian_benchmarks\n",
    "from bayesian_benchmarks.models import RegressionModel\n",
    "from bayesian_benchmarks.tasks.regression import run as run_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans2\n",
    "\n",
    "def init_inducing_points(X, num):\n",
    "    if X.shape[0] > num:\n",
    "        return kmeans2(X, num, minit='points')[0]\n",
    "    else:\n",
    "        return np.concatenate([X, np.random.randn(num - X.shape[0], X.shape[1])], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalLatentDeepGP_RegressionModel(RegressionModel):\n",
    "    \"\"\"\n",
    "    We wrap our Deep GP model in a RegressionModel class, to comply with\n",
    "    bayesian_benchmarks' interface. This means we need to implement:\n",
    "    - fit\n",
    "    - predict\n",
    "    - sample\n",
    "    \"\"\"\n",
    "    def __init__(self, is_test=False, seed=0):\n",
    "        super().__init__(is_test=is_test, seed=seed)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        print(\"X shape:\", X.shape)\n",
    "        print(\"Y shape:\", Y.shape)\n",
    "\n",
    "        class Config:\n",
    "            LATENT_DIM = 2\n",
    "            X_dim, Y_dim = X.shape[1], Y.shape[1]\n",
    "            D_in = X_dim + LATENT_DIM\n",
    "            OUTPUT_DIMS = [D_in, Y.shape[1]]\n",
    "            ADAM_LR = 0.01\n",
    "            if self.is_test:\n",
    "                M = 5\n",
    "                MAXITER = 10\n",
    "            else:\n",
    "                M = 500\n",
    "                MAXITER = int(10e3)\n",
    "            \n",
    "        print(Config.MAXITER)\n",
    "        \n",
    "        # Encder\n",
    "        encoder = gpflux.GPflowEncoder(Config.X_dim + Config.Y_dim, Config.LATENT_DIM, [50, 50])\n",
    "            \n",
    "        # Layer 1\n",
    "        Z1 = init_inducing_points(X, Config.M)\n",
    "        Z1 = np.concatenate([Z1, np.random.randn(Z1.shape[0], Config.LATENT_DIM)], 1)\n",
    "        feat1 = gpflow.features.InducingPoints(Z1)\n",
    "        kern1 = gpflow.kernels.RBF(Config.D_in, lengthscales=float(Config.D_in) ** 0.5, variance=0.1)\n",
    "        mean1 = gpflow.mean_functions.Identity(Config.D_in)\n",
    "        layer1 = gpflux.layers.GPLayer(kern1, feat1, Config.OUTPUT_DIMS[0])\n",
    "\n",
    "        # Layer 2\n",
    "        Z2 = np.random.randn(Config.M, Config.D_in)\n",
    "        feat2 = gpflow.features.InducingPoints(Z2)\n",
    "        kern2 = gpflow.kernels.RBF(Config.D_in, lengthscales=float(Config.D_in) ** 0.5)\n",
    "        mean2 = gpflow.mean_functions.Linear(np.random.randn(Config.D_in, Config.Y_dim))\n",
    "        layer2 = gpflux.layers.GPLayer(kern2, feat2, Config.OUTPUT_DIMS[0])\n",
    "\n",
    "        self.model = gpflux.ConditionalLatentDeepGP(X, Y, encoder, [layer1, layer2])\n",
    "\n",
    "        # minimize\n",
    "        print(\"before optimization:\", self.model.compute_log_likelihood())\n",
    "        gpflow.training.AdamOptimizer(Config.ADAM_LR).minimize(self.model, maxiter=Config.MAXITER)\n",
    "        print(\"after optimization:\", self.model.compute_log_likelihood())\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.decode(X)\n",
    "    \n",
    "    def sample(self, X, num_samples):\n",
    "        m, v = self.model.decode(X)\n",
    "        return m + np.random.randn(*m.shape) * np.sqrt(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    seed = 0\n",
    "    dataset = \"energy\"\n",
    "    split = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (691, 8)\n",
      "Y shape: (691, 1)\n",
      "10\n",
      "before optimization: -15509.312211619883\n",
      "after optimization: -13851.51257550061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__dict__': <attribute '__dict__' of 'ARGS' objects>,\n",
       " '__doc__': None,\n",
       " '__module__': '__main__',\n",
       " '__weakref__': <attribute '__weakref__' of 'ARGS' objects>,\n",
       " 'dataset': 'energy',\n",
       " 'seed': 0,\n",
       " 'split': 0,\n",
       " 'test_loglik': -1.4602694436489472,\n",
       " 'test_loglik_unnormalized': -3.771183126173496,\n",
       " 'test_mae': 0.9042866710630015,\n",
       " 'test_mae_unnormalized': 9.118495539208668,\n",
       " 'test_rmse': 0.9858913864032861,\n",
       " 'test_rmse_unnormalized': 9.941367595847593}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression(ARGS, is_test=True, write_to_database=False, Model=ConditionalLatentDeepGP_RegressionModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
