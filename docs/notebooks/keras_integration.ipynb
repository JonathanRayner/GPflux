{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Keras integration\n",
    "\n",
    "TODO: Some explanation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import gpflux\n",
    "from gpflow.ci_utils import ci_niter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpflux.layers import LikelihoodLoss\n",
    "\n",
    "tf.keras.backend.set_floatx(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"../../tests/snelson1d.npz\")\n",
    "X, Y = d[\"X\"], d[\"Y\"]\n",
    "num_data, input_dim = X.shape\n",
    "_, output_dim = Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(X, Y, \".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layers():\n",
    "    num_inducing = 13\n",
    "    hidden_dim = 1\n",
    "\n",
    "    init_kmeans = gpflux.initializers.KmeansInitializer(X, num_inducing)\n",
    "    layer1 = gpflux.helpers.construct_gp_layer(\n",
    "        num_data, num_inducing, input_dim, hidden_dim, initializer=init_kmeans\n",
    "    )\n",
    "    layer1.mean_function = gpflow.mean_functions.Identity()  # TODO: pass layer_type instead\n",
    "    layer1.q_sqrt.assign(layer1.q_sqrt * 0.01)\n",
    "\n",
    "    init_last_layer = gpflux.initializers.FeedForwardInitializer()\n",
    "    layer2 = gpflux.helpers.construct_gp_layer(\n",
    "        num_data, num_inducing, hidden_dim, output_dim, initializer=init_last_layer,\n",
    "    )\n",
    "\n",
    "    likelihood_layer = gpflux.layers.LikelihoodLayer(gpflow.likelihoods.Gaussian(0.01))\n",
    "\n",
    "    return layer1, layer2, likelihood_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_class):\n",
    "    layer1, layer2, likelihood_layer = create_layers()\n",
    "\n",
    "    inputs = tf.keras.Input((input_dim,))\n",
    "    f1 = layer1(inputs)\n",
    "    f2 = layer2(f1)\n",
    "    outputs = likelihood_layer(f2)\n",
    "\n",
    "    model = model_class(inputs=inputs, outputs=outputs)\n",
    "    return model, likelihood_layer.likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_epochs = ci_niter(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp, dgp_likelihood = create_model(tf.keras.Model)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\", patience=5, factor=0.95, verbose=1, min_lr=1e-6,\n",
    "    )\n",
    "]\n",
    "\n",
    "dgp.compile(tf.optimizers.Adam(learning_rate=0.1), loss=LikelihoodLoss(dgp_likelihood))\n",
    "\n",
    "history = dgp.fit(x=X, y=Y, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp_natgrad, dgp_natgrad_likelihood = create_model(gpflux.optimization.NatGradModel)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\", patience=5, factor=0.95, verbose=1, min_lr=1e-6,\n",
    "    )\n",
    "]\n",
    "\n",
    "dgp_natgrad.compile(\n",
    "    [\n",
    "        gpflow.optimizers.NaturalGradient(gamma=0.05),\n",
    "        gpflow.optimizers.NaturalGradient(gamma=0.05),\n",
    "        tf.optimizers.Adam(learning_rate=0.1),\n",
    "    ],\n",
    "    loss=LikelihoodLoss(dgp_natgrad_likelihood),\n",
    ")\n",
    "\n",
    "history_natgrad = dgp_natgrad.fit(\n",
    "    x=X, y=Y, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dgp(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y, \"x\")\n",
    "plt.errorbar(X.squeeze(), np.squeeze(res.y_mean), np.sqrt(np.squeeze(res.y_var)), ls=\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Adam\")\n",
    "plt.plot(history_natgrad.history[\"loss\"], label=\"NatGrad\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.4.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
