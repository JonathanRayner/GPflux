{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 3)\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gpflow.config import default_float\n",
    "from gpflow.kernels import RBF, Matern52\n",
    "\n",
    "from gpflux.layers.basis_functions.random_fourier_features import RandomFourierFeatures\n",
    "from gpflux.sampling.kernel_with_mercer_decomposition import KernelWithMercerDecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings that are fixed across experiments\n",
    "kernel_class = RBF  # choose alternatively kernel_class = Matern52\n",
    "noise_variance = 1e-3  # variance of the observation model\n",
    "num_test_samples = 256  # number of test samples for evaluation (1024 in the paper)\n",
    "num_experiment_runs = 32  # number of experiment repetitions (64 in the paper)\n",
    "\n",
    "# settings that vary across experiments\n",
    "num_input_dimensions = [2, 4, 8]  # number of input dimensions\n",
    "train_sample_exponents = [2, 4, 6, 8, 10]  # num_train_samples = 2 ** train_sample_exponents\n",
    "num_train_samples = [2 ** train_sample_exponent for train_sample_exponent in train_sample_exponents]\n",
    "l = [1024, 4096, 16384]  # the actual number of feature functions is l + num_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_analytic_GP_predictions(X, y, kernel, noise_variance, X_star):\n",
    "    \"\"\"\n",
    "    Identify the mean and covariance of an analytic GPR posterior for test point locations.\n",
    "    :param X: train point locations of shape [N x D]\n",
    "    :param y: train targets of shape [N x 1]\n",
    "    :param kernel: kernel object\n",
    "    :param noise_variance: variance of the observation model\n",
    "    :param X_star: test point locations of shape [N* x D]\n",
    "    :return: mean and covariance of the noise-free predictions of shape [N*] and [N* x N*] respectively\n",
    "    \"\"\"\n",
    "    gpr_model = GPR(data=(X, y), kernel=kernel, noise_variance=noise_variance)\n",
    "    \n",
    "    f_mean, f_var = gpr_model.predict_f(X_star, full_cov=True)\n",
    "    f_mean, f_var = f_mean[..., 0], f_var[0]\n",
    "    assert f_mean.shape == (X_star.shape[0],)\n",
    "    assert f_var.shape == (X_star.shape[0], X_star.shape[0])\n",
    "    \n",
    "    return f_mean, f_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Matheron_predictions(X, y, exact_kernel, approximate_kernel, noise_variance, X_star):\n",
    "    \"\"\"\n",
    "    Identify the mean and covariance using the Matheron approximation of the exact posterior.\n",
    "    :param X: train point locations of shape [N x D]\n",
    "    :param y: train targets of shape [N x 1]\n",
    "    :param exact_kernel: the exact kernel object\n",
    "    :param approximate_kernel: the approximate kernel object based on feature functions\n",
    "    :param noise_variance: variance of the observation model\n",
    "    :param X_star: test point locations of shape [N* x D]\n",
    "    :return: mean and covariance of the noise-free predictions of shape [N*] and [N* x N*] respectively\n",
    "    \"\"\"\n",
    "    phi_star = approximate_kernel._eigenfunctions(X_star)\n",
    "    assert phi_star.shape[0] == X_star.shape[0]\n",
    "    \n",
    "    phi = approximate_kernel._eigenfunctions(X)\n",
    "    assert phi.shape[0] == X.shape[0]\n",
    "\n",
    "    kXstarX = exact_kernel.K(X_star, X)\n",
    "    assert kXstarX.shape == (X_star.shape[0], X.shape[0])\n",
    "    \n",
    "    KXX = exact_kernel.K(X)\n",
    "    kXX_plus_noise_var = tf.linalg.set_diag(KXX, tf.linalg.diag_part(KXX) + noise_variance)\n",
    "    assert kXX_plus_noise_var.shape == (X.shape[0], X.shape[0])\n",
    "    \n",
    "    kXX_inv_mul_phi = tf.linalg.solve(kXX_plus_noise_var, phi)\n",
    "    assert kXX_inv_mul_phi.shape[0] == X.shape[0]\n",
    "    \n",
    "    kXX_inv_mul_y = tf.linalg.solve(kXX_plus_noise_var, y)\n",
    "    assert kXX_inv_mul_y.shape[0] == X.shape[0]\n",
    "\n",
    "    f_mean = kXstarX @ kXX_inv_mul_y\n",
    "    f_mean = f_mean[..., 0]\n",
    "    assert f_mean.shape[0] == X_star.shape[0]\n",
    "    \n",
    "    f_var_sqrt = phi_star - kXstarX @ kXX_inv_mul_phi\n",
    "    assert f_var_sqrt.shape[0] == X_star.shape[0]\n",
    "    \n",
    "    f_var = f_var_sqrt @ tf.transpose(f_var_sqrt)\n",
    "    assert f_var.shape == (X_star.shape[0], X_star.shape[0])\n",
    "    \n",
    "    return f_mean, f_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log10_Wasserstein_distance(mean, covariance, approximate_mean, approximate_covariance, jitter=1e-12):\n",
    "    \"\"\"\n",
    "    Identify the decadic logarithm of the Wasserstein distance based on the means and covariance matrices.\n",
    "    :param mean: analytic mean of shape [N*]\n",
    "    :param covariance: analytic covariance of shape [N* x N*]\n",
    "    :param approximate_mean: approximate mean of shape [N*]\n",
    "    :param approximate_covariance: approximate covariance of shape [N* x N*]\n",
    "    :param jitter: jitter value for numerical robustness\n",
    "    :return: a scalar log distance value\n",
    "    \"\"\"\n",
    "    mean_distance = tf.norm(mean - approximate_mean)\n",
    "    matrix_product = covariance @ approximate_covariance    \n",
    "    term = tf.linalg.sqrtm(tf.linalg.set_diag(matrix_product, tf.linalg.diag_part(matrix_product) + jitter))    \n",
    "    trace = tf.linalg.trace(covariance + approximate_covariance - 2.0 * term)\n",
    "    wasserstein_distance = mean_distance ** 2.0 + trace\n",
    "    log10_ws_distance = tf.math.log(wasserstein_distance) / tf.math.log(tf.constant(10.0, dtype=default_float()))\n",
    "    return log10_ws_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment(num_input_dimensions, num_train_samples, l):\n",
    "    \"\"\"\n",
    "    Compute the log10 Wassertein distance between the weight space approximated GP and the exact GP,\n",
    "    and between the Matheron approximated GP and the exact GP.\n",
    "    :param num_input_dimensions: number of input dimensions\n",
    "    :param num_train_samples: number of training samples\n",
    "    :param l: parameter to identify the number of feature functions\n",
    "    :return: the log10 Wasserstein distances for both approximations\n",
    "    \"\"\"\n",
    "    lengthscale = (num_input_dimensions / 100.0) ** 0.5  # adjust kernel lengthscale to the number of input dims\n",
    "    num_feature_functions = num_train_samples + l\n",
    "\n",
    "    # exact kernel\n",
    "    exact_kernel = kernel_class(lengthscales=lengthscale)\n",
    "\n",
    "    # weight space approximated kernel\n",
    "    feature_functions = RandomFourierFeatures(\n",
    "        kernel=kernel_class(lengthscales=lengthscale),\n",
    "        output_dim=num_feature_functions,\n",
    "        dtype=default_float()\n",
    "    )\n",
    "    feature_coefficients = np.ones((num_feature_functions, 1), dtype=default_float())\n",
    "    approximate_kernel = KernelWithMercerDecomposition(\n",
    "        kernel=None,\n",
    "        eigenfunctions=feature_functions,\n",
    "        eigenvalues=feature_coefficients\n",
    "    )\n",
    "\n",
    "    # training data set and test points for evaluation    \n",
    "    X = []\n",
    "    for i in range(num_input_dimensions):\n",
    "        random_samples = np.random.uniform(low=0.15, high=0.85, size=(num_train_samples,))\n",
    "        X.append(random_samples)\n",
    "    X = np.array(X).transpose()\n",
    "    \n",
    "    kXX = exact_kernel.K(X)\n",
    "    kXX_plus_noise_var = tf.linalg.set_diag(kXX, tf.linalg.diag_part(kXX) + noise_variance)\n",
    "    lXX = tf.linalg.cholesky(kXX_plus_noise_var)\n",
    "    y = tf.matmul(lXX, tf.random.normal([num_train_samples, 1], dtype=X.dtype))\n",
    "    \n",
    "    X_star = []\n",
    "    for i in range(num_input_dimensions):\n",
    "        random_samples = np.random.uniform(low=0.0, high=0.3, size=(num_test_samples,))\n",
    "        indices = np.random.uniform(size=(num_test_samples,)) < 0.5\n",
    "        random_samples[indices] = np.random.uniform(low=0.7, high=1.0, size=(num_test_samples,))[indices]\n",
    "        X_star.append(random_samples)\n",
    "    X_star = np.array(X_star).transpose()\n",
    "\n",
    "    # identify mean and covariance of the analytic GPR posterior\n",
    "    f_mean_exact, f_var_exact = compute_analytic_GP_predictions(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        kernel=exact_kernel,\n",
    "        noise_var=noise_variance,\n",
    "        X_star=X_star\n",
    "    )\n",
    "\n",
    "    # identify mean and covariance of the analytic GPR posterior when using the weight space approximated kernel\n",
    "    f_mean_approx, f_var_approx = compute_analytic_GP_predictions(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        kernel=approximate_kernel,\n",
    "        noise_var=noise_variance,\n",
    "        X_star=X_star\n",
    "    )\n",
    "\n",
    "    # identify mean and covariance using the Matheron approximation\n",
    "    f_mean_matheron, f_var_matheron = compute_Matheron_predictions(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        exact_kernel=exact_kernel,\n",
    "        approximate_kernel=approximate_kernel,\n",
    "        noise_var=noise_variance,\n",
    "        X_star=X_star\n",
    "    )\n",
    "\n",
    "    # compute log10 Wasserstein distance between the exact solution and the weight space approximation\n",
    "    log10_ws_dist_approx = log10_wasserstein_distance(\n",
    "        f_mean_exact,\n",
    "        f_var_exact,\n",
    "        f_mean_approx,\n",
    "        f_var_approx\n",
    "    )\n",
    "\n",
    "    # compute log10 Wassertein distance between the exact solution and the Matheron approximation\n",
    "    log10_ws_dist_matheron = log10_wasserstein_distance(\n",
    "        f_mean_exact,\n",
    "        f_var_exact,\n",
    "        f_mean_matheron,\n",
    "        f_var_matheron\n",
    "    )\n",
    "\n",
    "    # return the log Wasserstein distances for both approximations\n",
    "    return log10_ws_dist_approx, log10_ws_dist_matheron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment_for_multiple_runs(num_input_dimensions, num_train_samples, l):\n",
    "    \"\"\"\n",
    "    Conduct the experiment as specified above `num_experiment_runs` times and identify the quartiles for\n",
    "    the log10 Wassertein distance between the weight space approximated GP and the exact GP, \n",
    "    and between the Matheron approximated GP and the exact GP.\n",
    "    :param num_input_dimensions: number of input dimensions\n",
    "    :param num_train_samples: number of training samples\n",
    "    :param l: parameter to identify the number of feature functions\n",
    "    :return: the quartiles of the log10 Wasserstein distance for both approximations\n",
    "    \"\"\"\n",
    "    list_of_log10_ws_dist_approx = []  # for the analytic solution using the weight space approximated kernel\n",
    "    list_of_log10_ws_dist_matheron = []  # for the Matheron approximation\n",
    "    for _ in range(num_experiment_runs):\n",
    "        log10_ws_dist_approx, log10_ws_dist_matheron = conduct_experiment(\n",
    "            num_input_dimensions=num_input_dimensions,\n",
    "            num_train_samples=num_train_samples,\n",
    "            l=l\n",
    "        )\n",
    "        list_of_log10_ws_dist_approx.append(log10_ws_dist_approx)\n",
    "        list_of_log10_ws_dist_matheron.append(log10_ws_dist_matheron)\n",
    "\n",
    "    log10_ws_dist_approx_quarts = np.quantile(list_of_log10_ws_dist_approx, q=(0.25, 0.5, 0.75))\n",
    "    log10_ws_dist_matheron_quarts = np.quantile(list_of_log10_ws_dist_matheron, q=(0.25, 0.5, 0.75))\n",
    "    return log10_ws_dist_approx_quarts, log10_ws_dist_matheron_quarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment_for_different_train_data_sizes(num_input_dimensions, l):\n",
    "    \"\"\"\n",
    "    Conduct the experiment as specified above for different trainig data set sizes and store the results in lists.\n",
    "    :param num_input_dimensions: number of input dimensions\n",
    "    :param l: parameter to identify the number of feature functions\n",
    "    :return: lists of quartiles of the log10 Wasserstein distance for both approximations\n",
    "    \"\"\"\n",
    "    list_log10_ws_dist_approx_quarts = []  # for the analytic solution using the weight space approximated kernel\n",
    "    list_log10_ws_dist_matheron_quarts = []  # for the Matheron approximation\n",
    "    for nts in num_train_samples:\n",
    "        log10_ws_dist_approx_quarts, log10_ws_dist_matheron_quarts = conduct_experiment_for_multiple_runs(\n",
    "            num_input_dimensions=num_input_dimensions,\n",
    "            num_train_samples=nts,\n",
    "            l=l\n",
    "        )\n",
    "        print('Completed for d = ' + str(num_input_dimensions) + ' and l = ' + str(l) +\n",
    "              ' and num train samples = ' + str(num_train_samples))\n",
    "        list_log10_ws_dist_approx_quarts.append(log10_ws_dist_approx_quarts)\n",
    "        list_log10_ws_dist_matheron_quarts.append(log10_ws_dist_matheron_quarts)\n",
    "\n",
    "    list_log10_ws_dist_approx_quarts = np.array(list_log10_ws_dist_approx_quarts).transpose()\n",
    "    list_log10_ws_dist_matheron_quarts = np.array(list_log10_ws_dist_matheron_quarts).transpose()\n",
    "    return list_log10_ws_dist_approx_quarts, list_log10_ws_dist_matheron_quarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment_for_different_l(num_input_dimensions):\n",
    "    \"\"\"\n",
    "    Conduct the experiment as specified above for different l (that determins the number of feature functions)\n",
    "    and store the results in lists of lists.\n",
    "    :param num_input_dimensions: number of input dimensions\n",
    "    :return: lists of lists of quartiles of the log10 Wasserstein distance for both approximations\n",
    "    \"\"\"\n",
    "    list_of_approx_results = []  # for the analytic solution using the weight space approximated kernel\n",
    "    list_of_matheron_results = []  # for the Matheron approximation\n",
    "    for _l in l:\n",
    "        approx_results, matheron_results = conduct_experiment_for_different_train_data_sizes(\n",
    "            num_input_dimensions=num_input_dimensions,\n",
    "            l=_l\n",
    "        )\n",
    "        print()\n",
    "        list_of_approx_results.append(approx_results)\n",
    "        list_of_matheron_results.append(matheron_results)\n",
    "    return list_of_approx_results, list_of_matheron_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plots\n",
    "fig, axs = plt.subplots(1, len(num_input_dimensions))\n",
    "for i in range(len(num_input_dimensions)):\n",
    "    axs[i].set_title('Number of input dimensions $D=' + str(num_input_dimensions[i]) + '$')\n",
    "    axs[i].set_xlabel('Number of training data points $N$')\n",
    "    axs[i].set_xscale('log')\n",
    "axs[0].set_ylabel('$\\log_{10}$ Wasserstein distance')\n",
    "\n",
    "# conduct experiments and plot results\n",
    "for i in range(len(num_input_dimensions))  # iterate through the different number of input dimensions\n",
    "    approx_results, matheron_results = conduct_experiment_for_different_l(\n",
    "        num_input_dimensions=num_input_dimensions[i]\n",
    "    )\n",
    "    \n",
    "    colors = ['bisque', 'orange', 'peru']\n",
    "    for j in range(len(approx_results)):\n",
    "        approx_result = approx_results[j]\n",
    "        axs[i].fill_between(\n",
    "            num_train_samples, approx_results[0], approx_results[2], color=colors[j], alpha=0.1\n",
    "        )\n",
    "        axs[i].plot(num_train_samples, approx_results[1], 'o', color=colors[j])\n",
    "        axs[i].plot(num_train_samples, approx_results[1], color=colors[j], linewidth=0.5)\n",
    "\n",
    "    colors = ['lightblue', 'blue', 'darkblue']\n",
    "    for j in range(len(matheron_results)):\n",
    "        matheron_result = matheron_results[j]\n",
    "        axs[i].fill_between(\n",
    "            num_train_samples, matheron_results[0], matheron_results[2], color=colors[j], alpha=0.1\n",
    "        )\n",
    "        axs[i].plot(num_train_samples, matheron_results[1], 'o', color=colors[j])\n",
    "        axs[i].plot(num_train_samples, matheron_results[1], color=colors[], linewidth=0.5)\n",
    "\n",
    "# show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
